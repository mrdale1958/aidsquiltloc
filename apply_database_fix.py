#!/usr/bin/env python3
"""
AIDS Memorial Quilt Database Fix - Direct Schema Patch
Following project coding standards with comprehensive error handling and type safety
Implements separation of concerns for digital humanities research applications
"""

import re
from pathlib import Path
from typing import Dict, Any

def apply_database_schema_fix() -> None:
    """
    Apply direct schema compatibility fix to src/database.py
    Implements comprehensive error handling with specific exception types
    Following AIDS Memorial Quilt Records coding standards
    """
    print("üîß AIDS Memorial Quilt Database Schema Fix")
    print("=" * 42)
    print("Following digital humanities research standards with comprehensive error handling\n")
    
    database_py_path = Path("src/database.py")
    
    if not database_py_path.exists():
        print("‚ùå src/database.py file not found")
        return
    
    try:
        # Read the current database.py file
        with open(database_py_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print("‚úÖ Read src/database.py successfully")
        
        # Find the problematic SQL query in get_records method
        # The error shows it's around line 270, looking for metadata column
        problematic_pattern = r'SELECT block_id, title, description, created_date, total_panels,\s*metadata,\s*scraped_at, updated_at'
        
        if re.search(problematic_pattern, content):
            print("‚úÖ Found problematic SQL query with 'metadata' column")
            
            # Replace with schema-compatible query
            corrected_query = """SELECT id, block_id as item_id, title, description, 
                       CASE 
                           WHEN metadata_json IS NOT NULL AND metadata_json != '{}' 
                           THEN json_extract(metadata_json, '$.subjects')
                           ELSE NULL 
                       END as subjects,
                       CASE 
                           WHEN metadata_json IS NOT NULL AND metadata_json != '{}' 
                           THEN json_extract(metadata_json, '$.names')
                           ELSE NULL 
                       END as names,
                       created_date as dates,
                       NULL as url, NULL as image_url, NULL as content_hash,
                       scraped_at as created_at, updated_at"""
            
            # Replace the problematic query
            content = re.sub(
                problematic_pattern,
                corrected_query,
                content
            )
            print("‚úÖ Replaced problematic SQL query")
        
        else:
            # Look for alternative patterns that might be causing the issue
            alternative_patterns = [
                r'SELECT.*metadata.*FROM.*quilt_blocks',
                r'cursor\.execute\(\s*"""\s*SELECT.*metadata',
            ]
            
            for pattern in alternative_patterns:
                if re.search(pattern, content, re.DOTALL):
                    print(f"‚úÖ Found alternative problematic pattern")
                    # More aggressive replacement for the entire get_records method
                    get_records_replacement = '''async def get_records(self, limit: int = 20, offset: int = 0) -> List[Dict[str, Any]]:
        """
        Get AIDS Memorial Quilt records with pagination (SCHEMA-COMPATIBLE VERSION)
        Implements error resilience and digital humanities research standards
        Addresses metadata_json column compatibility for quilt_blocks table
        
        Args:
            limit: Maximum number of records to return (1-1000)
            offset: Number of records to skip (>=0)
            
        Returns:
            List of record dictionaries compatible with API format
            
        Raises:
            DatabaseConnectionError: If database connection fails
            DataValidationError: If parameters are invalid
        """
        try:
            if not self.connection:
                raise DatabaseConnectionError("Database not initialized")
            
            # Validate parameters following project error handling guidelines
            if limit <= 0 or limit > 1000:
                raise DataValidationError("Limit must be between 1 and 1000")
            if offset < 0:
                raise DataValidationError("Offset must be non-negative")
            
            # Determine which table to use for primary data source
            table_counts = {}
            for table_name in ["collection_items", "quilt_blocks", "quilt_panels"]:
                try:
                    cursor = await self.connection.execute(f"SELECT COUNT(*) FROM {table_name}")
                    result = await cursor.fetchone()
                    table_counts[table_name] = result[0] if result else 0
                    await cursor.close()
                except Exception:
                    table_counts[table_name] = 0
            
            # Choose primary table based on data availability
            if table_counts.get("collection_items", 0) > 0:
                primary_table = "collection_items"
            elif table_counts.get("quilt_blocks", 0) >= table_counts.get("quilt_panels", 0):
                primary_table = "quilt_blocks"
            else:
                primary_table = "quilt_panels"
            
            logger.info(f"AIDS Memorial Quilt Database: Attempting to fetch records from {primary_table} (limit: {limit}, offset: {offset})")
            
            # Log table counts for debugging
            for table_name, count in table_counts.items():
                logger.info(f"AIDS Memorial Quilt Database: Table {table_name} contains {count} rows")
            
            logger.info(f"AIDS Memorial Quilt Database: Fetching from {primary_table} table")
            
            # Use schema-compatible queries based on actual table structure
            if primary_table == "collection_items":
                cursor = await self.connection.execute("""
                    SELECT id, item_id, title, description, subjects, names, dates,
                           url, image_url, content_hash, created_at, updated_at
                    FROM collection_items
                    ORDER BY created_at DESC
                    LIMIT ? OFFSET ?
                """, (limit, offset))
            elif primary_table == "quilt_blocks":
                cursor = await self.connection.execute("""
                    SELECT id, block_id as item_id, title, description, 
                           CASE 
                               WHEN metadata_json IS NOT NULL AND metadata_json != '{}' 
                               THEN json_extract(metadata_json, '$.subjects')
                               ELSE NULL 
                           END as subjects,
                           CASE 
                               WHEN metadata_json IS NOT NULL AND metadata_json != '{}' 
                               THEN json_extract(metadata_json, '$.names')
                               ELSE NULL 
                           END as names,
                           created_date as dates,
                           NULL as url, NULL as image_url, NULL as content_hash,
                           scraped_at as created_at, updated_at
                    FROM quilt_blocks
                    ORDER BY id DESC
                    LIMIT ? OFFSET ?
                """, (limit, offset))
            else:  # quilt_panels
                cursor = await self.connection.execute("""
                    SELECT id, panel_id as item_id, title, description,
                           CASE 
                               WHEN metadata_json IS NOT NULL AND metadata_json != '{}' 
                               THEN json_extract(metadata_json, '$.subjects')
                               ELSE NULL 
                           END as subjects,
                           CASE 
                               WHEN metadata_json IS NOT NULL AND metadata_json != '{}' 
                               THEN json_extract(metadata_json, '$.names')
                               ELSE NULL 
                           END as names,
                           scraped_at as dates,
                           image_urls as url, image_urls as image_url, NULL as content_hash,
                           scraped_at as created_at, updated_at
                    FROM quilt_panels
                    ORDER BY id DESC
                    LIMIT ? OFFSET ?
                """, (limit, offset))
            
            rows = await cursor.fetchall()
            await cursor.close()
            
            # Convert to API-compatible format following digital humanities standards
            columns = ["id", "item_id", "title", "description", "subjects", 
                      "names", "dates", "url", "image_url", "content_hash", 
                      "created_at", "updated_at"]
            
            records = []
            for row in rows:
                record = dict(zip(columns, row))
                
                # Parse JSON fields safely for AIDS Memorial Quilt metadata preservation
                for json_field in ["subjects", "names"]:
                    field_value = record.get(json_field)
                    if field_value and field_value != "NULL" and field_value is not None:
                        try:
                            if isinstance(field_value, str):
                                if field_value.startswith('[') or field_value.startswith('{'):
                                    import json
                                    parsed_value = json.loads(field_value)
                                    record[json_field] = parsed_value if isinstance(parsed_value, list) else [str(parsed_value)]
                                else:
                                    record[json_field] = [field_value]
                            else:
                                record[json_field] = [str(field_value)] if field_value else []
                        except Exception as parse_error:
                            logger.warning(f"AIDS Memorial Quilt Database: Error parsing {json_field}: {parse_error}")
                            record[json_field] = []
                    else:
                        record[json_field] = []
                
                # Handle image URLs for Library of Congress attribution
                for url_field in ["url", "image_url"]:
                    url_value = record.get(url_field)
                    if url_value and url_value != "NULL" and url_value is not None:
                        try:
                            if isinstance(url_value, str) and url_value.startswith('['):
                                import json
                                urls_list = json.loads(url_value)
                                record[url_field] = urls_list[0] if isinstance(urls_list, list) and urls_list else None
                        except Exception:
                            pass
                    else:
                        record[url_field] = None
                
                # Handle dates for Library of Congress attribution
                dates_value = record.get("dates")
                if dates_value and dates_value != "NULL":
                    try:
                        if isinstance(dates_value, str) and not dates_value.startswith('['):
                            record["dates"] = [dates_value]
                        elif isinstance(dates_value, str) and dates_value.startswith('['):
                            import json
                            record["dates"] = json.loads(dates_value)
                        else:
                            record["dates"] = [str(dates_value)] if dates_value else []
                    except Exception:
                        record["dates"] = []
                else:
                    record["dates"] = []
                
                # Ensure string fields are properly handled
                for string_field in ["title", "description"]:
                    if record.get(string_field) is None:
                        record[string_field] = ""
                
                records.append(record)
            
            logger.info(f"AIDS Memorial Quilt Database: Retrieved {len(records)} records from {primary_table}")
            return records
            
        except Exception as e:
            logger.error(f"AIDS Memorial Quilt Database: Critical error getting records: {e}")
            logger.error(f"AIDS Memorial Quilt Database: Full traceback: {traceback.format_exc()}")
            raise DatabaseConnectionError(f"Failed to get records from AIDS Memorial Quilt database: {e}")'''
                    
                    # Replace the entire get_records method
                    method_pattern = r'async def get_records\(self.*?\n        except Exception as e:.*?raise DatabaseConnectionError.*?\n'
                    if re.search(method_pattern, content, re.DOTALL):
                        content = re.sub(method_pattern, get_records_replacement, content, flags=re.DOTALL)
                        print("‚úÖ Replaced entire get_records method")
                    break
            else:
                print("‚ö†Ô∏è  Could not find the exact problematic query pattern")
                print("Let me create a complete replacement...")
                
                # If we can't find the exact pattern, let's add the import we need and append a corrected method
                if 'import traceback' not in content:
                    content = content.replace('import json', 'import json\nimport traceback')
                
                # Add the corrected method at the end of the class
                class_end_pattern = r'(\s+async def close\(self\).*?\n        logger\.info.*?\n)'
                if re.search(class_end_pattern, content, re.DOTALL):
                    replacement = r'\1\n' + get_records_replacement + '\n'
                    content = re.sub(class_end_pattern, replacement, content, flags=re.DOTALL)
                    print("‚úÖ Added corrected get_records method")
        
        # Create backup of original file
        backup_path = database_py_path.with_suffix('.py.backup')
        with open(backup_path, 'w', encoding='utf-8') as f:
            with open(database_py_path, 'r', encoding='utf-8') as original:
                f.write(original.read())
        print(f"‚úÖ Created backup: {backup_path}")
        
        # Write the corrected content
        with open(database_py_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print("‚úÖ Applied schema compatibility fix to src/database.py")
        print("\nüéØ Next Steps:")
        print("1. Restart API server: python api_server.py")
        print("2. Test fix: python recordstest.py") 
        print("3. Verify React dashboard displays records")
        print(f"4. If issues occur, restore from backup: {backup_path}")
        
        print("\nüí° Fix Applied:")
        print("‚Ä¢ ‚úÖ Replaced 'metadata' with 'metadata_json' column references")
        print("‚Ä¢ ‚úÖ Added JSON extraction for subjects/names fields")
        print("‚Ä¢ ‚úÖ Implemented intelligent table selection logic")
        print("‚Ä¢ ‚úÖ Added comprehensive error handling for AIDS Memorial Quilt data")
        print("‚Ä¢ ‚úÖ Maintained Library of Congress attribution standards")
        
    except Exception as e:
        print(f"‚ùå Error applying fix: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    apply_database_schema_fix()